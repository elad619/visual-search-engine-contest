{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Process the Unsplash dataset with CLIP\n",
    "\n",
    "This notebook processes all the downloaded photos using OpenAI's [CLIP neural network](https://github.com/openai/CLIP). For each image we get a feature vector containing 512 float numbers, which we will store in a file. These feature vectors will be used later to compare them to the text feature vectors.\n",
    "\n",
    "This step will be significantly faster if you have a GPU, but it will also work on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load the photos\n",
    "\n",
    "Load all photos from the folder they were stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos found: 177\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the path to the photos\n",
    "dataset_version = \"lite\"  # Use \"lite\" or \"full\"\n",
    "photos_path = Path(r\"C:\\Users\\elad6\\OneDrive\\Documents\\NLI\\images_db\\images\")\n",
    "# List all JPGs in the folder\n",
    "photos_files = list(photos_path.rglob(\"*.jpg\"))\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Photos found: {len(photos_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load the CLIP net\n",
    "\n",
    "Load the CLIP net and define the function that computes the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the open CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Function that computes the feature vectors for a batch of images\n",
    "def compute_clip_features(photos_batch):\n",
    "    # Load all the photos from the files\n",
    "    photos = [Image.open(photo_file) for photo_file in photos_batch]\n",
    "    \n",
    "    # Preprocess all photos\n",
    "    photos_preprocessed = torch.stack([preprocess(photo) for photo in photos]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode the photos batch to compute the feature vectors and normalize them\n",
    "        photos_features = model.encode_image(photos_preprocessed)\n",
    "        photos_features /= photos_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Transfer the feature vectors back to the CPU and convert to numpy\n",
    "    return photos_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Process all photos\n",
    "\n",
    "Now we need to compute the features for all photos. We will do that in batches, because it is much more efficient. You should tune the batch size so that it fits on your GPU. The processing on the GPU is fairly fast, so the bottleneck will probably be loading the photos from the disk.\n",
    "\n",
    "In this step the feature vectors and the photo IDs of each batch will be saved to a file separately. This makes the whole process more robust. We will merge the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elad6\\anaconda3\\envs\\visual-search-engine-contest\\lib\\site-packages\\PIL\\Image.py:3035: DecompressionBombWarning: Image size (177712075 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/12\n",
      "Processing batch 3/12\n",
      "Processing batch 4/12\n",
      "Processing batch 5/12\n",
      "Processing batch 6/12\n",
      "Processing batch 7/12\n",
      "Processing batch 8/12\n",
      "Processing batch 9/12\n",
      "Processing batch 10/12\n",
      "Processing batch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elad6\\anaconda3\\envs\\visual-search-engine-contest\\lib\\site-packages\\PIL\\Image.py:3035: DecompressionBombWarning: Image size (138453540 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 12/12\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the batch size so that it fits on your GPU. You can also do the processing on the CPU, but it will be slower.\n",
    "batch_size = 16\n",
    "\n",
    "# Path where the feature vectors will be stored\n",
    "features_path = Path(r\"C:\\Users\\elad6\\OneDrive\\Documents\\NLI\\images_db\\features\")\n",
    "\n",
    "# Compute how many batches are needed\n",
    "batches = math.ceil(len(photos_files) / batch_size)\n",
    "\n",
    "# Process each batch\n",
    "for i in range(batches):\n",
    "    print(f\"Processing batch {i+1}/{batches}\")\n",
    "\n",
    "    batch_ids_path = features_path / f\"{i:010d}.csv\"\n",
    "    batch_features_path = features_path / f\"{i:010d}.npy\"\n",
    "    \n",
    "    # Only do the processing if the batch wasn't processed yet\n",
    "    if not batch_features_path.exists():\n",
    "        try:\n",
    "            # Select the photos for the current batch\n",
    "            batch_files = photos_files[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "            # Compute the features and save to a numpy file\n",
    "            batch_features = compute_clip_features(batch_files)\n",
    "            np.save(batch_features_path, batch_features)\n",
    "\n",
    "            # Save the photo IDs to a CSV file\n",
    "            photo_ids = [photo_file.name.split(\".\")[0] for photo_file in batch_files]\n",
    "            photo_ids_data = pd.DataFrame(photo_ids, columns=['photo_id'])\n",
    "            photo_ids_data.to_csv(batch_ids_path, index=False)\n",
    "        except Exception as e:\n",
    "            # Catch problems with the processing to make the process more robust\n",
    "            print(f'Problem with batch {i}: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Merge the features and the photo IDs. The resulting files are `features.npy` and `photo_ids.csv`. Feel free to delete the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load all numpy files\n",
    "features_list = [np.load(features_file) for features_file in sorted(features_path.glob(\"*.npy\"))]\n",
    "\n",
    "# Concatenate the features and store in a merged file\n",
    "features = np.concatenate(features_list)\n",
    "np.save(features_path / \"features.npy\", features)\n",
    "\n",
    "# Load all the photo IDs\n",
    "photo_ids = pd.concat([pd.read_csv(ids_file) for ids_file in sorted(features_path.glob(\"*.csv\"))])\n",
    "photo_ids.to_csv(features_path / \"photo_ids.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}